{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWJL/f9/qD0oocHPGp+U9/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aj1365/TransUNetplus2/blob/main/TransUNet%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0TIrzQR4798"
      },
      "outputs": [],
      "source": [
        "import cv2 # For CV operations\n",
        "from PIL import Image  #To create and store images\n",
        "import numpy as np\n",
        "\n",
        "#To binarize the input\n",
        "import h5py\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "metadata": {
        "id": "5HnxVxXi5OwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from osgeo import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "import rioxarray as rxr"
      ],
      "metadata": {
        "id": "D4ipnWKG5QpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"E:/AMAZON/\"\n",
        "\n",
        "\n",
        "# Ingest images and normalise\n",
        "\n",
        "## Training images\n",
        "training_images_list = os.listdir(r\"{}Training/image/\".format(base_dir))\n",
        "training_masks_list = []\n",
        "training_images = []\n",
        "\n",
        "for n in training_images_list:\n",
        "    \n",
        "    training_masks_list.append(n)\n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Training/image/{}\".format(base_dir,n))))\n",
        "    a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "    training_images.append(a)\n",
        "\n",
        "## Training masks\n",
        "training_masks = []\n",
        "for n in training_masks_list:\n",
        "    \n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Training/label/{}\".format(base_dir,n))))\n",
        "    training_masks.append(a)\n",
        "\n",
        "## Test images\n",
        "test_images_list = os.listdir(r\"{}Test/image/\".format(base_dir))\n",
        "test_masks_list = []\n",
        "test_images = []\n",
        "\n",
        "for n in test_images_list:\n",
        "    \n",
        "    test_masks_list.append(n)\n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Test/image/{}\".format(base_dir,n))))\n",
        "    a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "    test_images.append(a)\n",
        "\n",
        "## Test masks\n",
        "test_masks = []\n",
        "for n in test_masks_list:\n",
        "    \n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Test/mask/{}\".format(base_dir,n))))\n",
        "    test_masks.append(a)\n",
        "\n",
        "## Validation images\n",
        "validation_images_list = os.listdir(r\"{}Validation/images/\".format(base_dir))\n",
        "validation_masks_list = []\n",
        "validation_images = []\n",
        "\n",
        "for n in validation_images_list:\n",
        "    \n",
        "    validation_masks_list.append(n)\n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Validation/images/{}\".format(base_dir,n))))\n",
        "    a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "    validation_images.append(a)\n",
        "\n",
        "## Validation masks\n",
        "validation_masks = []\n",
        "for n in validation_masks_list:\n",
        "    \n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Validation/masks/{}\".format(base_dir,n))))\n",
        "    validation_masks.append(a)\n",
        "    \n",
        "\n",
        "# Show example train image\n",
        "plt.imshow((np.array(rxr.open_rasterio(r\"{}Training/image/{}\".format(base_dir,training_images_list[11])))[0,:,:]))\n",
        "\n",
        "# Pre-process data, reshaping and transposing\n",
        "for i in range(len(training_images)):\n",
        "    \n",
        "    training_images[i] = training_images[i].astype('float32')\n",
        "    training_images[i] = training_images[i].T\n",
        "\n",
        "for i in range(len(training_masks)):\n",
        "    \n",
        "    training_masks[i] = training_masks[i].reshape(128,128)\n",
        "    training_masks[i] = training_masks[i].T\n",
        "\n",
        "for i in range(len(validation_images)):\n",
        "    \n",
        "    validation_images[i] = validation_images[i].astype('float32')\n",
        "    validation_images[i] = validation_images[i].T\n",
        "\n",
        "for i in range(len(validation_masks)):\n",
        "    \n",
        "    validation_masks[i] = validation_masks[i].reshape(128,128)\n",
        "    validation_masks[i] = validation_masks[i].T\n",
        "\n",
        "for i in range(len(test_images)):\n",
        "    \n",
        "    test_images[i] = test_images[i].astype('float32')\n",
        "    test_images[i] = test_images[i].T\n",
        "\n",
        "for i in range(len(test_masks)):\n",
        "    \n",
        "    test_masks[i] = test_masks[i].reshape(128,128)\n",
        "    test_masks[i] = test_masks[i].T\n",
        "\n",
        "for i in range(len(training_images)):\n",
        "    \n",
        "    training_images[i] = training_images[i].reshape(128,128,4)\n",
        "\n",
        "for i in range(len(validation_images)):\n",
        "    \n",
        "    validation_images[i] = validation_images[i].reshape(128,128,4)\n",
        "\n",
        "for i in range(len(test_images)):\n",
        "    \n",
        "    test_images[i] = test_images[i].reshape(128,128,4)\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "9JGPkcfm5UXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_dir2 = \"E:/ATLANTIC FOREST/\"\n",
        "\n",
        "## Training images\n",
        "training_images_list2 = os.listdir(r\"{}Training/image/\".format(base_dir2))\n",
        "training_masks_list2 = []\n",
        "training_images2 = []\n",
        "for n in training_images_list2:\n",
        "    \n",
        "    training_masks_list2.append(n)\n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Training/image/{}\".format(base_dir2,n))))\n",
        "    a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "    training_images2.append(a)\n",
        "\n",
        "## Training masks\n",
        "training_masks2 = []\n",
        "for n in training_masks_list2:\n",
        "    \n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Training/label/{}\".format(base_dir2,n))))\n",
        "    training_masks2.append(a)\n",
        "\n",
        "## Test images\n",
        "test_images_list2 = os.listdir(r\"{}Test/image/\".format(base_dir2))\n",
        "test_masks_list2 = []\n",
        "test_images2 = []\n",
        "for n in test_images_list2:\n",
        "    \n",
        "    test_masks_list2.append(n)\n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Test/image/{}\".format(base_dir2,n))))\n",
        "    a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "    test_images2.append(a)\n",
        "\n",
        "## Test masks\n",
        "test_masks2 = []\n",
        "for n in test_masks_list2:\n",
        "    \n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Test/mask/{}\".format(base_dir2,n))))\n",
        "    test_masks2.append(a)\n",
        "\n",
        "## Validation images\n",
        "validation_images_list2 = os.listdir(r\"{}Validation/images/\".format(base_dir2))\n",
        "validation_masks_list2 = []\n",
        "validation_images2 = []\n",
        "for n in validation_images_list2:\n",
        "    \n",
        "    validation_masks_list2.append(n)\n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Validation/images/{}\".format(base_dir2,n))))\n",
        "    a = (a-np.min(a)) / (np.max(a)-np.min(a))\n",
        "    validation_images2.append(a)\n",
        "\n",
        "## Validation masks\n",
        "validation_masks2 = []\n",
        "for n in validation_masks_list2:\n",
        "    \n",
        "    a = (np.array(rxr.open_rasterio(r\"{}Validation/masks/{}\".format(base_dir2,n))))\n",
        "    validation_masks2.append(a)"
      ],
      "metadata": {
        "id": "fRWoSXlb5ZJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Show example train image\n",
        "plt.imshow((np.array(rxr.open_rasterio(r\"{}Training/image/{}\".format(base_dir2,training_images_list2[11])))[0,:,:]))"
      ],
      "metadata": {
        "id": "_JbchANK5hw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNW5WfuA5kOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBoqgPtJ5kR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lq3apZf5kVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process data, reshaping and transposing\n",
        "for i in range(len(training_images2)):\n",
        "    \n",
        "    training_images2[i] = training_images2[i].astype('float32')\n",
        "    training_images2[i] = training_images2[i].T\n",
        "\n",
        "for i in range(len(training_masks2)):\n",
        "    \n",
        "    training_masks2[i] = training_masks2[i].reshape(128,128)\n",
        "    training_masks2[i] = training_masks2[i].T\n",
        "\n",
        "for i in range(len(validation_images2)):\n",
        "    \n",
        "    validation_images2[i] = validation_images2[i].astype('float32')\n",
        "    validation_images2[i] = validation_images2[i].T\n",
        "\n",
        "for i in range(len(validation_masks2)):\n",
        "    \n",
        "    validation_masks2[i] = validation_masks2[i].reshape(128,128)\n",
        "    validation_masks2[i] = validation_masks2[i].T\n",
        "\n",
        "for i in range(len(test_images2)):\n",
        "    \n",
        "    test_images2[i] = test_images2[i].astype('float32')\n",
        "    test_images2[i] = test_images2[i].T\n",
        "\n",
        "for i in range(len(test_masks2)):\n",
        "    \n",
        "    test_masks2[i] = test_masks2[i].reshape(128,128)\n",
        "    test_masks2[i] = test_masks2[i].T\n",
        "\n",
        "for i in range(len(training_images2)):\n",
        "    \n",
        "    training_images2[i] = training_images2[i].reshape(128,128,4)\n",
        "\n",
        "for i in range(len(validation_images2)):\n",
        "    \n",
        "    validation_images2[i] = validation_images2[i].reshape(128,128,4)\n",
        "\n",
        "for i in range(len(test_images2)):\n",
        "    \n",
        "    test_images2[i] = test_images2[i].reshape(128,128,4)"
      ],
      "metadata": {
        "id": "l-ol4y-c5kY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Produce generators for training images\n",
        "\n",
        "TrainX = np.stack(training_images)\n",
        "TrainY = np.stack(training_masks)\n",
        "\n",
        "ValX = np.stack(validation_images)\n",
        "ValY = np.stack(validation_masks)\n",
        "\n",
        "\n",
        "TestX = np.stack(test_images)\n",
        "TestY = np.stack(test_masks)\n"
      ],
      "metadata": {
        "id": "w0nTddPc5q2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainX.shape,TrainY.shape,TestX.shape,ValX.shape,ValY.shape"
      ],
      "metadata": {
        "id": "wc_epGlY5w71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(TrainX[0,:,:,0:3]*5)"
      ],
      "metadata": {
        "id": "-sP_kKjg5xGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.min(TestY), np.max(TestY)"
      ],
      "metadata": {
        "id": "l0-PEYbo5xKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('*' * 30)\n",
        "print('Saving Ground Truth masks to files...')\n",
        "print('*' * 30)\n",
        "GT_dir = 'E:/GT'\n",
        "if not os.path.exists(GT_dir):\n",
        "    os.mkdir(GT_dir)\n",
        "for i, image in enumerate(TestY):\n",
        "    image = (image).astype(np.uint8)\n",
        "    image=image*255\n",
        "    cv2.imwrite(os.path.join(GT_dir, str(i + 1) + '.png'), image)"
      ],
      "metadata": {
        "id": "04RhMEBt5xOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('*' * 30)\n",
        "print('Saving Ground Truth masks to files...')\n",
        "print('*' * 30)\n",
        "XGT_dir = 'E:/XGT'\n",
        "if not os.path.exists(XGT_dir):\n",
        "    os.mkdir(XGT_dir)\n",
        "for i, image in enumerate(TestX):\n",
        "    image = (image[:,:,0:3].astype(np.uint8))\n",
        "    #cv2.imwrite(os.path.join(XGT_dir, str(i + 1) + '_pred.png'), image)"
      ],
      "metadata": {
        "id": "5ZGu8k2d5xR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(\"E:/Dataset_train.h5\", 'w') as hdf:\n",
        "    hdf.create_dataset('images', data=TrainX, compression='gzip', compression_opts=9)\n",
        "    hdf.create_dataset('masks', data=TrainY, compression='gzip', compression_opts=9)"
      ],
      "metadata": {
        "id": "dEUF__zk6Gm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xsP8B_OC6GqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TransUNet Model for Deforestation**"
      ],
      "metadata": {
        "id": "2d9fXECM6N_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "import keras\n",
        "import keras.callbacks\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import backend as keras\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "tQFX4qds6GuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "hyId0WSC6Gw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from typing import Callable"
      ],
      "metadata": {
        "id": "P62JJ05A6Gz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics to be used when evaluating the network\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "precision = tf.keras.metrics.Precision()\n",
        "recall = tf.keras.metrics.Recall()\n",
        "f1 = F1Score(num_classes=2, name='f1', average='micro', threshold=0.4)\n",
        "sgd_optimizer = Adam()"
      ],
      "metadata": {
        "id": "GBwUSGE15q6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DiceLoss(targets, inputs, smooth=1e-6):\n",
        "\n",
        "    \n",
        "    intersection = K.sum(targets *inputs)\n",
        "    dice = (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n",
        "    return 1 - dice"
      ],
      "metadata": {
        "id": "Du0HArRP5q-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IoULoss(targets, inputs, smooth=1e-6):\n",
        "    \n",
        "\n",
        "    \n",
        "    intersection = K.sum(targets *inputs)\n",
        "    total = K.sum(targets) + K.sum(inputs)\n",
        "    union = total - intersection\n",
        "    \n",
        "    IoU = (intersection + smooth) / (union + smooth)\n",
        "    return 1 - IoU"
      ],
      "metadata": {
        "id": "Vvp5FQbz6g_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "660d3v9v6hCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model creation**"
      ],
      "metadata": {
        "id": "K6Ez9oE56l6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tfk.layers\n",
        "tfm = tf.math\n",
        "L2_WEIGHT_DECAY = 1e-4"
      ],
      "metadata": {
        "id": "211X5FkZ6hFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "from keras_unet_collection.layer_utils import *\n",
        "from keras_unet_collection.activations import GELU, Snake\n",
        "from keras_unet_collection._model_unet_2d import UNET_left, UNET_right\n",
        "from keras_unet_collection.transformer_layers import patch_extract, patch_embedding\n",
        "from keras_unet_collection._backbone_zoo import backbone_zoo, bach_norm_checker\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, MultiHeadAttention, LayerNormalization, Dense, Embedding\n"
      ],
      "metadata": {
        "id": "Vm8y-Rxj6hJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def  HetConv(feature_map, conv_filter,groups):\n",
        "    \n",
        "    # Groupwise Convolution\n",
        "    x1=Conv2D(filters=conv_filter, kernel_size=(3,3), groups=groups, padding='same')(feature_map)\n",
        "     \n",
        "    # Pointwise Convolution\n",
        "    x2= Conv2D(filters=conv_filter, kernel_size=(1,1), strides=1, padding='same')(feature_map)\n",
        "    \n",
        "    \n",
        "    addition = Add()([x1, x2])\n",
        "   \n",
        "    return addition\n",
        "\n",
        "\n",
        "def CONV_stack(X, channel, kernel_size=3, stack_num=2, \n",
        "               dilation_rate=1, activation='ReLU', \n",
        "               batch_norm=False, name='conv_stack'):\n",
        "    \n",
        "    bias_flag = not batch_norm\n",
        "    \n",
        "    # stacking Convolutional layers\n",
        "    for i in range(stack_num):\n",
        "        \n",
        "        activation_func = eval(activation)\n",
        "        \n",
        "        X = HetConv(X, conv_filter=channel, groups=4)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=3, name='{}_{}_bn'.format(name, i))(X)\n",
        "        \n",
        "        # activation\n",
        "        activation_func = eval(activation)\n",
        "        X = activation_func(name='{}_{}_activation'.format(name, i))(X)\n",
        "        \n",
        "    return X\n",
        "\n",
        "\n",
        "def encode_layer(X, channel, pool_size, pool, kernel_size='auto', \n",
        "                 activation='ReLU', batch_norm=False, name='encode'):\n",
        "\n",
        "    # parsers\n",
        "    if (pool in [False, True, 'max', 'ave']) is not True:\n",
        "        raise ValueError('Invalid pool keyword')\n",
        "        \n",
        "    # maxpooling2d as default\n",
        "    if pool is True:\n",
        "        pool = 'max'\n",
        "        \n",
        "    elif pool is False:\n",
        "        # stride conv configurations\n",
        "        bias_flag = not batch_norm\n",
        "    \n",
        "    if pool == 'max':\n",
        "        X = MaxPooling2D(pool_size=(pool_size, pool_size), name='{}_maxpool'.format(name))(X)\n",
        "        \n",
        "    elif pool == 'ave':\n",
        "        X = AveragePooling2D(pool_size=(pool_size, pool_size), name='{}_avepool'.format(name))(X)\n",
        "        \n",
        "    else:\n",
        "        if kernel_size == 'auto':\n",
        "            kernel_size = pool_size\n",
        "        \n",
        "        # linear convolution with strides\n",
        "        X = Conv2D(channel, kernel_size, strides=(pool_size, pool_size), \n",
        "                   padding='valid', use_bias=bias_flag, name='{}_stride_conv'.format(name))(X)\n",
        "        \n",
        "                \n",
        "           \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=3, name='{}_bn'.format(name))(X)\n",
        "            \n",
        "        # activation\n",
        "        if activation is not None:\n",
        "            activation_func = eval(activation)\n",
        "            X = activation_func(name='{}_activation'.format(name))(X)\n",
        "            \n",
        "    return X\n",
        "\n",
        "\n",
        "def attention_gate(X, g, channel,  \n",
        "                   activation='ReLU', \n",
        "                   attention='add', name='att'):\n",
        "\n",
        "    activation_func = eval(activation)\n",
        "    attention_func = eval(attention)\n",
        "    \n",
        "    # mapping the input tensor to the intermediate channel\n",
        "    theta_att = Conv2D(channel, 1, use_bias=True, name='{}_theta_x'.format(name))(X)\n",
        "    \n",
        "    # mapping the gate tensor\n",
        "    phi_g = Conv2D(channel, 1, use_bias=True, name='{}_phi_g'.format(name))(g)\n",
        "    \n",
        "    # ----- attention learning ----- #\n",
        "    query = attention_func([theta_att, phi_g], name='{}_add'.format(name))\n",
        "    \n",
        "    # nonlinear activation\n",
        "    f = activation_func(name='{}_activation'.format(name))(query)\n",
        "    \n",
        "    # linear transformation\n",
        "    psi_f = Conv2D(1, 1, use_bias=True, name='{}_psi_f'.format(name))(f)\n",
        "    # ------------------------------ #\n",
        "    \n",
        "    # sigmoid activation as attention coefficients\n",
        "    coef_att = Activation('sigmoid', name='{}_sigmoid'.format(name))(psi_f)\n",
        "    \n",
        "    # multiplicative attention masking\n",
        "    X_att = multiply([X, coef_att], name='{}_masking'.format(name))\n",
        "    \n",
        "    return X_att\n",
        "\n",
        "\n",
        "def UNET_left(X, channel, kernel_size=3, stack_num=2, activation='ReLU', \n",
        "              pool=True, batch_norm=False, name='left0'):\n",
        "\n",
        "    pool_size = 2\n",
        "\n",
        "    \n",
        "    X = encode_layer(X, channel, pool_size, pool, activation=activation, \n",
        "                     batch_norm=batch_norm, name='{}_encode'.format(name))\n",
        "\n",
        "    X = CONV_stack(X, channel, kernel_size, stack_num=stack_num, activation=activation, \n",
        "                   batch_norm=batch_norm, name='{}_conv'.format(name))\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def UNET_att_right(X, X_left, channel, att_channel, kernel_size=3, stack_num=2,\n",
        "                   activation='ReLU', atten_activation='ReLU', attention='add',\n",
        "                   unpool=True, batch_norm=False, name='right0'):\n",
        "    \n",
        "    X=  HetConv(X, conv_filter=channel, groups=4)\n",
        "    \n",
        "    pool_size = 2\n",
        "    \n",
        "    X = decode_layer(X, channel, pool_size, unpool, \n",
        "                     activation=activation, batch_norm=batch_norm, name='{}_decode'.format(name))\n",
        "    \n",
        "    X_left = attention_gate(X=X_left, g=X, channel=att_channel, activation=atten_activation, \n",
        "                            attention=attention, name='{}_att'.format(name))\n",
        "    \n",
        "    # Tensor concatenation\n",
        "    H = concatenate([X, X_left], axis=-1, name='{}_concat'.format(name))\n",
        "    \n",
        "    # stacked linear convolutional layers after concatenation\n",
        "    H = CONV_stack(H, channel, kernel_size, stack_num=stack_num, activation=activation, \n",
        "                   batch_norm=batch_norm, name='{}_conv_after_concat'.format(name))\n",
        "    \n",
        "    \n",
        "    return H\n",
        "    \n",
        "def ViT_MLP(X, filter_num, activation='GELU', name='MLP'):\n",
        "\n",
        "    activation_func = eval(activation)\n",
        "    \n",
        "    for i, f in enumerate(filter_num):\n",
        "        X = Dense(f, name='{}_dense_{}'.format(name, i))(X)\n",
        "        X = activation_func(name='{}_activation_{}'.format(name, i))(X)\n",
        "        \n",
        "    return X\n",
        "    \n",
        "def ViT_block(V, num_heads, key_dim, filter_num_MLP, activation='GELU', name='ViT'):\n",
        "\n",
        "    # Multiheaded self-attention (MSA)\n",
        "    V_atten = V # <--- skip\n",
        "    V_atten = LayerNormalization(name='{}_layer_norm_1'.format(name))(V_atten)\n",
        "    V_atten = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, \n",
        "                                 name='{}_atten'.format(name))(V_atten, V_atten)\n",
        "    # Skip connection\n",
        "    V_add = add([V_atten, V], name='{}_skip_1'.format(name)) # <--- skip\n",
        "    \n",
        "    # MLP\n",
        "    V_MLP = V_add # <--- skip\n",
        "    V_MLP = LayerNormalization(name='{}_layer_norm_2'.format(name))(V_MLP)\n",
        "    V_MLP = ViT_MLP(V_MLP, filter_num_MLP, activation, name='{}_mlp'.format(name))\n",
        "    # Skip connection\n",
        "    V_out = add([V_MLP, V_add, V], name='{}_skip_2'.format(name)) # <--- skip\n",
        "    \n",
        "    return V_out\n",
        "\n",
        "\n",
        "def transunet_plus2_base(input_tensor, filter_num, stack_num_down=2, stack_num_up=2, \n",
        "                      embed_dim=768, num_mlp=3072, num_heads=12, num_transformer=12,\n",
        "                      activation='ReLU', atten_activation='ReLU', attention='add', mlp_activation='GELU', batch_norm=False, pool=True, unpool=True, \n",
        "                      backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='transunet_plus2'):\n",
        "\n",
        "    activation_func = eval(activation)\n",
        "    \n",
        "    X_skip = []\n",
        "    depth_ = len(filter_num)\n",
        "    \n",
        "    # ----- internal parameters ----- #\n",
        "    \n",
        "    # patch size (fixed to 1-by-1)\n",
        "    patch_size = 1\n",
        "    \n",
        "    # input tensor size\n",
        "    input_size = input_tensor.shape[1]\n",
        "    \n",
        "    # encoded feature map size\n",
        "    encode_size = input_size // 2**(depth_-1)\n",
        "    \n",
        "    # number of size-1 patches\n",
        "    num_patches = encode_size ** 2 \n",
        "    \n",
        "    # dimension of the attention key (= dimension of embedings)\n",
        "    key_dim = embed_dim\n",
        "    \n",
        "    # number of MLP nodes\n",
        "    filter_num_MLP = [num_mlp, embed_dim]\n",
        "    \n",
        "    # ----- UNet-like downsampling ----- #\n",
        "    \n",
        "    # no backbone cases\n",
        "    if backbone is None:\n",
        "\n",
        "        X = input_tensor\n",
        "\n",
        "        \n",
        "        X = HetConv(X, conv_filter=filter_num[0], groups=4)\n",
        "        X_skip.append(X)\n",
        "\n",
        "        # downsampling blocks\n",
        "        for i, f in enumerate(filter_num[1:]):\n",
        "            X = UNET_left(X, f, stack_num=stack_num_down, activation=activation, pool=pool, \n",
        "                          batch_norm=batch_norm, name='{}_down{}'.format(name, i+1))        \n",
        "            X_skip.append(X)\n",
        "\n",
        "    # backbone cases\n",
        "    else:\n",
        "        # handling VGG16 and VGG19 separately\n",
        "        if 'VGG' in backbone:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_skip = backbone_([input_tensor,])\n",
        "            depth_encode = len(X_skip)\n",
        "            \n",
        "        # for other backbones\n",
        "        else:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_-1, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_skip = backbone_([input_tensor,])\n",
        "            depth_encode = len(X_skip) + 1\n",
        "\n",
        "\n",
        "        # extra conv2d blocks are applied\n",
        "        # if downsampling levels of a backbone < user-specified downsampling levels\n",
        "        if depth_encode < depth_:\n",
        "\n",
        "            # begins at the deepest available tensor  \n",
        "            X = X_skip[-1]\n",
        "\n",
        "            # extra downsamplings\n",
        "            for i in range(depth_-depth_encode):\n",
        "                i_real = i + depth_encode\n",
        "\n",
        "                X = UNET_left(X, filter_num[i_real], stack_num=stack_num_down, activation=activation, pool=pool, \n",
        "                              batch_norm=batch_norm, name='{}_down{}'.format(name, i_real+1))\n",
        "                X_skip.append(X)\n",
        "        \n",
        "    # subtrack the last tensor (will be replaced by the ViT output)\n",
        "    X = X_skip[-1]\n",
        "    X_skip = X_skip[:-1]\n",
        "\n",
        "    # 1-by-1 linear transformation before entering ViT blocks\n",
        "   \n",
        "\n",
        "    X = HetConv(X, conv_filter=filter_num[-1], groups=4)\n",
        "\n",
        "    X = patch_extract((patch_size, patch_size))(X)\n",
        "    X = patch_embedding(num_patches, embed_dim)(X)\n",
        "\n",
        "    # stacked ViTs \n",
        "    for i in range(num_transformer):\n",
        "        X = ViT_block(X, num_heads, key_dim, filter_num_MLP, activation=mlp_activation, \n",
        "                      name='{}_ViT_{}'.format(name, i))\n",
        "\n",
        "    # reshape patches to feature maps\n",
        "    X = tf.reshape(X, (-1, encode_size, encode_size, embed_dim))\n",
        "\n",
        "    X = HetConv(X, conv_filter=filter_num[-1], groups=4)\n",
        "\n",
        "    X_skip.append(X)\n",
        "    \n",
        "    # ----- UNet-like upsampling ----- #\n",
        "    \n",
        "    # reverse indexing encoded feature maps\n",
        "    X_skip = X_skip[::-1]\n",
        "    # upsampling begins at the deepest available tensor\n",
        "    X = X_skip[0]\n",
        "    # other tensors are preserved for concatenation\n",
        "    X_decode = X_skip[1:]\n",
        "    depth_decode = len(X_decode)\n",
        "\n",
        "    # reverse indexing filter numbers\n",
        "    filter_num_decode = filter_num[:-1][::-1]\n",
        "\n",
        "    # upsampling with concatenation\n",
        "    for i in range(depth_decode):\n",
        "        \n",
        "        f = filter_num_decode[i]\n",
        "              \n",
        "        X = UNET_att_right(X, X_decode[i], f, att_channel=f//2, stack_num=stack_num_up,\n",
        "                           activation=activation, atten_activation=atten_activation, attention=attention,\n",
        "                           unpool=unpool, batch_norm=batch_norm, name='{}_up{}'.format(name, i))\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "    # if tensors for concatenation is not enough\n",
        "    # then use upsampling without concatenation \n",
        "    if depth_decode < depth_-1:\n",
        "        for i in range(depth_-depth_decode-1):\n",
        "            i_real = i + depth_decode\n",
        "            X = UNET_right(X, None, filter_num_decode[i_real], stack_num=stack_num_up, activation=activation, \n",
        "                       unpool=unpool, batch_norm=batch_norm, concat=False, name='{}_up{}'.format(name, i_real))\n",
        "            \n",
        "    return X\n",
        "\n",
        "def transunet_plus2(input_size, filter_num, n_labels, stack_num_down=1, stack_num_up=1,\n",
        "                 embed_dim=44, num_mlp = 252, num_heads=4, num_transformer=1,\n",
        "                 activation='ReLU',atten_activation='ReLU',attention='add', mlp_activation='GELU', output_activation='Sigmoid', batch_norm=False, pool=True, unpool=True, \n",
        "                 backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='transunet_plus2'):\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "        \n",
        "    IN = Input(input_size)\n",
        "    \n",
        "    # base    \n",
        "    X = transunet_plus2_base(IN, filter_num, stack_num_down=stack_num_down, stack_num_up=stack_num_up, \n",
        "                          embed_dim=embed_dim, num_mlp=num_mlp, num_heads=num_heads, num_transformer=num_transformer,\n",
        "                          activation=activation, atten_activation=atten_activation, attention=attention, mlp_activation=mlp_activation, batch_norm=batch_norm, pool=pool, unpool=unpool,\n",
        "                          backbone=backbone, weights=weights, freeze_backbone=freeze_backbone, freeze_batch_norm=freeze_batch_norm, name=name)\n",
        "    \n",
        "    # output layer\n",
        "    OUT = CONV_output(X, n_labels, kernel_size=1, activation=output_activation, name='{}_output'.format(name))\n",
        "    \n",
        "    # functional API model\n",
        "    model = Model(inputs=[IN,], outputs=[OUT,], name='{}_model'.format(name))\n",
        "    model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy', precision, recall, f1])\n",
        "#   \n",
        "    \n",
        "    return model\n"
      ],
      "metadata": {
        "id": "6eGShCJ66hMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=transunet_plus2(input_size=(128, 128, 4), filter_num=[16,32,64,128], n_labels=1)"
      ],
      "metadata": {
        "id": "XbMVQbt35rBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('*'*30)\n",
        "print('Loading and preprocessing train data...')\n",
        "print('*'*30)\n",
        "file = h5py.File('E:/Dataset_train.h5', 'r')\n",
        "imgs_train = file.get('images')\n",
        "imgs_mask_train = file.get('masks')\n",
        "imgs_train = np.array(imgs_train)\n",
        "imgs_mask_train = np.array(imgs_mask_train)\n",
        "\n",
        "print(imgs_train.shape)\n",
        "print(imgs_mask_train.shape)\n",
        "#imgs_train = imgs_train.reshape(1108,256,256,1)\n",
        "\n",
        " \n",
        "imgs_train = imgs_train.astype('float32')\n",
        "\n",
        "mean = np.mean(imgs_train)  # mean for data centering\n",
        "std = np.std(imgs_train)  # std for data normalization\n",
        "\n",
        "imgs_train -= mean\n",
        "imgs_train /= std\n",
        "\n",
        "imgs_mask_train = imgs_mask_train.astype('float32')\n",
        "#imgs_mask_train /= 255  # scale masks to [0, 1]\n",
        "\n",
        "print('*'*30)\n",
        "print('Creating and compiling model...')\n",
        "print('*'*30)\n",
        "model = transunet_plus2(input_size=(128, 128, 4), filter_num=[16, 32, 64, 128], n_labels=1)"
      ],
      "metadata": {
        "id": "dTqK9EQa6z8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay = 0.0001\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "s3NkB3rc6z_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpdg709T60DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ValX = np.array(ValX)\n",
        "ValX = ValX.astype('float32')\n",
        "ValX -= mean\n",
        "ValX /= std"
      ],
      "metadata": {
        "id": "U7kECpoT60GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "learning_rate=1e-3\n",
        "optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "\n",
        "checkpoint_filepath = \"E:/TransUnet++-Amazon.h5\"\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "        x=imgs_train,\n",
        "        y=imgs_mask_train,\n",
        "        batch_size=1,\n",
        "        epochs=40,\n",
        "        validation_data = (ValX, ValY)\n",
        "    )"
      ],
      "metadata": {
        "id": "2SeXhMCa60Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WsSN6cjG8L35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def _crps_tf(y_true, y_pred, factor=0.05):\n",
        "    \n",
        "    '''\n",
        "    core of (pseudo) CRPS loss.\n",
        "    \n",
        "    y_true: two-dimensional arrays\n",
        "    y_pred: two-dimensional arrays\n",
        "    factor: importance of std term\n",
        "    '''\n",
        "    \n",
        "    # mean absolute error\n",
        "    mae = K.mean(tf.abs(y_pred - y_true))\n",
        "    \n",
        "    dist = tf.math.reduce_std(y_pred)\n",
        "    \n",
        "    return mae - factor*dist\n",
        "\n",
        "def crps2d_tf(y_true, y_pred, factor=0.05):\n",
        "    \n",
        "    '''\n",
        "    (Experimental)\n",
        "    An approximated continuous ranked probability score (CRPS) loss function:\n",
        "    \n",
        "        CRPS = mean_abs_err - factor * std\n",
        "        \n",
        "    * Note that the \"real CRPS\" = mean_abs_err - mean_pairwise_abs_diff\n",
        "    \n",
        "     Replacing mean pairwise absolute difference by standard deviation offers\n",
        "     a complexity reduction from O(N^2) to O(N*logN) \n",
        "    \n",
        "    ** factor > 0.1 may yield negative loss values.\n",
        "    \n",
        "    Compatible with high-level Keras training methods\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        y_true: training target with shape=(batch_num, x, y, 1)\n",
        "        y_pred: a forward pass with shape=(batch_num, x, y, 1)\n",
        "        factor: relative importance of standard deviation term.\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\n",
        "    \n",
        "    y_pred = tf.squeeze(y_pred)\n",
        "    y_true = tf.squeeze(y_true)\n",
        "    \n",
        "    batch_num = y_pred.shape.as_list()[0]\n",
        "    \n",
        "    crps_out = 0\n",
        "    for i in range(batch_num):\n",
        "        crps_out += _crps_tf(y_true[i, ...], y_pred[i, ...], factor=factor)\n",
        "        \n",
        "    return crps_out/batch_num\n",
        "\n",
        "\n",
        "def _crps_np(y_true, y_pred, factor=0.05):\n",
        "    \n",
        "    '''\n",
        "    Numpy version of _crps_tf.\n",
        "    '''\n",
        "    \n",
        "    # mean absolute error\n",
        "    mae = np.nanmean(np.abs(y_pred - y_true))\n",
        "    dist = np.nanstd(y_pred)\n",
        "    \n",
        "    return mae - factor*dist\n",
        "\n",
        "def crps2d_np(y_true, y_pred, factor=0.05):\n",
        "    \n",
        "    '''\n",
        "    (Experimental)\n",
        "    Nunpy version of `crps2d_tf`.\n",
        "    \n",
        "    Documentation refers to `crps2d_tf`.\n",
        "    '''\n",
        "    \n",
        "    y_true = np.squeeze(y_true)\n",
        "    y_pred = np.squeeze(y_pred)\n",
        "    \n",
        "    batch_num = len(y_pred)\n",
        "    \n",
        "    crps_out = 0\n",
        "    for i in range(batch_num):\n",
        "        crps_out += _crps_np(y_true[i, ...], y_pred[i, ...], factor=factor)\n",
        "        \n",
        "    return crps_out/batch_num\n",
        "\n",
        "# ========================= #\n",
        "# Dice loss and variants\n",
        "\n",
        "def dice_coef(y_true, y_pred, const=K.epsilon()):\n",
        "    '''\n",
        "    Sørensen–Dice coefficient for 2-d samples.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        y_true, y_pred: predicted outputs and targets.\n",
        "        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    # flatten 2-d tensors\n",
        "    y_true_pos = tf.reshape(y_true, [-1])\n",
        "    y_pred_pos = tf.reshape(y_pred, [-1])\n",
        "    \n",
        "    # get true pos (TP), false neg (FN), false pos (FP).\n",
        "    true_pos  = tf.reduce_sum(y_true_pos * y_pred_pos)\n",
        "    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n",
        "    false_pos = tf.reduce_sum((1-y_true_pos) * y_pred_pos)\n",
        "    \n",
        "    # 2TP/(2TP+FP+FN) == 2TP/()\n",
        "    coef_val = (2.0 * true_pos + const)/(2.0 * true_pos + false_pos + false_neg)\n",
        "    \n",
        "    return coef_val\n",
        "\n",
        "def dice(y_true, y_pred, const=K.epsilon()):\n",
        "    '''\n",
        "    Sørensen–Dice Loss.\n",
        "    \n",
        "    dice(y_true, y_pred, const=K.epsilon())\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n",
        "        \n",
        "    '''\n",
        "    # tf tensor casting\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\n",
        "    \n",
        "    # <--- squeeze-out length-1 dimensions.\n",
        "    y_pred = tf.squeeze(y_pred)\n",
        "    y_true = tf.squeeze(y_true)\n",
        "    \n",
        "    loss_val = 1 - dice_coef(y_true, y_pred, const=const)\n",
        "    \n",
        "    return loss_val\n",
        "\n",
        "# ========================= #\n",
        "# Tversky loss and variants\n",
        "\n",
        "def tversky_coef(y_true, y_pred, alpha=0.5, const=K.epsilon()):\n",
        "    '''\n",
        "    Weighted Sørensen–Dice coefficient.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        y_true, y_pred: predicted outputs and targets.\n",
        "        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    # flatten 2-d tensors\n",
        "    y_true_pos = tf.reshape(y_true, [-1])\n",
        "    y_pred_pos = tf.reshape(y_pred, [-1])\n",
        "    \n",
        "    # get true pos (TP), false neg (FN), false pos (FP).\n",
        "    true_pos  = tf.reduce_sum(y_true_pos * y_pred_pos)\n",
        "    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n",
        "    false_pos = tf.reduce_sum((1-y_true_pos) * y_pred_pos)\n",
        "    \n",
        "    # TP/(TP + a*FN + b*FP); a+b = 1\n",
        "    coef_val = (true_pos + const)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + const)\n",
        "    \n",
        "    return coef_val\n",
        "\n",
        "def tversky(y_true, y_pred, alpha=0.5, const=K.epsilon()):\n",
        "    '''\n",
        "    Tversky Loss.\n",
        "    \n",
        "    tversky(y_true, y_pred, alpha=0.5, const=K.epsilon())\n",
        "    \n",
        "    ----------\n",
        "    Hashemi, S.R., Salehi, S.S.M., Erdogmus, D., Prabhu, S.P., Warfield, S.K. and Gholipour, A., 2018. \n",
        "    Tversky as a loss function for highly unbalanced image segmentation using 3d fully convolutional deep networks. \n",
        "    arXiv preprint arXiv:1803.11078.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        alpha: tunable parameter within [0, 1]. Alpha handles imbalance classification cases.\n",
        "        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n",
        "        \n",
        "    '''\n",
        "    # tf tensor casting\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\n",
        "    \n",
        "    # <--- squeeze-out length-1 dimensions.\n",
        "    y_pred = tf.squeeze(y_pred)\n",
        "    y_true = tf.squeeze(y_true)\n",
        "    \n",
        "    loss_val = 1 - tversky_coef(y_true, y_pred, alpha=alpha, const=const)\n",
        "    \n",
        "    return loss_val\n",
        "\n",
        "def focal_tversky(y_true, y_pred, alpha=0.5, gamma=4/3, const=K.epsilon()):\n",
        "    \n",
        "    '''\n",
        "    Focal Tversky Loss (FTL)\n",
        "    \n",
        "    focal_tversky(y_true, y_pred, alpha=0.5, gamma=4/3)\n",
        "    \n",
        "    ----------\n",
        "    Abraham, N. and Khan, N.M., 2019, April. A novel focal tversky loss function with improved \n",
        "    attention u-net for lesion segmentation. In 2019 IEEE 16th International Symposium on Biomedical Imaging \n",
        "    (ISBI 2019) (pp. 683-687). IEEE.\n",
        "    \n",
        "    ----------\n",
        "    Input\n",
        "        alpha: tunable parameter within [0, 1]. Alpha handles imbalance classification cases \n",
        "        gamma: tunable parameter within [1, 3].\n",
        "        const: a constant that smooths the loss gradient and reduces numerical instabilities.\n",
        "        \n",
        "    '''\n",
        "    # tf tensor casting\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\n",
        "    \n",
        "    # <--- squeeze-out length-1 dimensions.\n",
        "    y_pred = tf.squeeze(y_pred)\n",
        "    y_true = tf.squeeze(y_true)\n",
        "    \n",
        "    # (Tversky loss)**(1/gamma) \n",
        "    loss_val = tf.math.pow((1-tversky_coef(y_true, y_pred, alpha=alpha, const=const)), 1/gamma)\n",
        "    \n",
        "    return loss_val\n",
        "\n",
        "# ========================= #\n",
        "# MS-SSIM\n",
        "\n",
        "def ms_ssim(y_true, y_pred, **kwargs):\n",
        "    \"\"\"\n",
        "    Multiscale structural similarity (MS-SSIM) loss.\n",
        "    \n",
        "    ms_ssim(y_true, y_pred, **tf_ssim_kw)\n",
        "    \n",
        "    ----------\n",
        "    Wang, Z., Simoncelli, E.P. and Bovik, A.C., 2003, November. Multiscale structural similarity for image quality assessment. \n",
        "    In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003 (Vol. 2, pp. 1398-1402). Ieee.\n",
        "    \n",
        "    ----------\n",
        "    Input\n",
        "        kwargs: keywords of `tf.image.ssim_multiscale`\n",
        "                https://www.tensorflow.org/api_docs/python/tf/image/ssim_multiscale\n",
        "                \n",
        "        *Issues of `tf.image.ssim_multiscale`refers to:\n",
        "                https://stackoverflow.com/questions/57127626/error-in-calculation-of-inbuilt-ms-ssim-function-in-tensorflow\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\n",
        "    \n",
        "    y_pred = tf.squeeze(y_pred)\n",
        "    y_true = tf.squeeze(y_true)\n",
        "    \n",
        "    tf_ms_ssim = tf.image.ssim_multiscale(y_true, y_pred, **kwargs)\n",
        "        \n",
        "    return 1 - tf_ms_ssim\n",
        "\n",
        "# ======================== #\n",
        "\n",
        "def iou_box_coef(y_true, y_pred, mode='giou', dtype=tf.float32):\n",
        "    \n",
        "    \"\"\"\n",
        "    Inersection over Union (IoU) and generalized IoU coefficients for bounding boxes.\n",
        "    \n",
        "    iou_box_coef(y_true, y_pred, mode='giou', dtype=tf.float32)\n",
        "    \n",
        "    ----------\n",
        "    Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I. and Savarese, S., 2019. \n",
        "    Generalized intersection over union: A metric and a loss for bounding box regression. \n",
        "    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 658-666).\n",
        "    \n",
        "    ----------\n",
        "    Input\n",
        "        y_true: the target bounding box. \n",
        "        y_pred: the predicted bounding box.\n",
        "        \n",
        "        Elements of a bounding box should be organized as: [y_min, x_min, y_max, x_max].\n",
        "        mode: 'iou' for IoU coeff (i.e., Jaccard index);\n",
        "              'giou' for generalized IoU coeff.\n",
        "        \n",
        "        dtype: the data type of input tensors.\n",
        "               Default is tf.float32.\n",
        "    \"\"\"\n",
        "    \n",
        "    zero = tf.convert_to_tensor(0.0, dtype)\n",
        "    \n",
        "    # subtrack bounding box coords\n",
        "    ymin_true, xmin_true, ymax_true, xmax_true = tf.unstack(y_true, 4, axis=-1)\n",
        "    ymin_pred, xmin_pred, ymax_pred, xmax_pred = tf.unstack(y_pred, 4, axis=-1)\n",
        "    \n",
        "    # true area\n",
        "    w_true = tf.maximum(zero, xmax_true - xmin_true)\n",
        "    h_true = tf.maximum(zero, ymax_true - ymin_true)\n",
        "    area_true = w_true * h_true\n",
        "    \n",
        "    # pred area\n",
        "    w_pred = tf.maximum(zero, xmax_pred - xmin_pred)\n",
        "    h_pred = tf.maximum(zero, ymax_pred - ymin_pred)\n",
        "    area_pred = w_pred * h_pred\n",
        "    \n",
        "    # intersections\n",
        "    intersect_ymin = tf.maximum(ymin_true, ymin_pred)\n",
        "    intersect_xmin = tf.maximum(xmin_true, xmin_pred)\n",
        "    intersect_ymax = tf.minimum(ymax_true, ymax_pred)\n",
        "    intersect_xmax = tf.minimum(xmax_true, xmax_pred)\n",
        "    \n",
        "    w_intersect = tf.maximum(zero, intersect_xmax - intersect_xmin)\n",
        "    h_intersect = tf.maximum(zero, intersect_ymax - intersect_ymin)\n",
        "    area_intersect = w_intersect * h_intersect\n",
        "    \n",
        "    # IoU\n",
        "    area_union = area_true + area_pred - area_intersect\n",
        "    iou = tf.math.divide_no_nan(area_intersect, area_union)\n",
        "    \n",
        "    if mode == \"iou\":\n",
        "        \n",
        "        return iou\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        # encolsed coords\n",
        "        enclose_ymin = tf.minimum(ymin_true, ymin_pred)\n",
        "        enclose_xmin = tf.minimum(xmin_true, xmin_pred)\n",
        "        enclose_ymax = tf.maximum(ymax_true, ymax_pred)\n",
        "        enclose_xmax = tf.maximum(xmax_true, xmax_pred)\n",
        "        \n",
        "        # enclosed area\n",
        "        w_enclose = tf.maximum(zero, enclose_xmax - enclose_xmin)\n",
        "        h_enclose = tf.maximum(zero, enclose_ymax - enclose_ymin)\n",
        "        area_enclose = w_enclose * h_enclose\n",
        "        \n",
        "        # generalized IoU\n",
        "        giou = iou - tf.math.divide_no_nan((area_enclose - area_union), area_enclose)\n",
        "\n",
        "        return giou\n",
        "\n",
        "def iou_box(y_true, y_pred, mode='giou', dtype=tf.float32):\n",
        "    \"\"\"\n",
        "    Inersection over Union (IoU) and generalized IoU losses for bounding boxes. \n",
        "    \n",
        "    iou_box(y_true, y_pred, mode='giou', dtype=tf.float32)\n",
        "    \n",
        "    ----------\n",
        "    Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I. and Savarese, S., 2019. \n",
        "    Generalized intersection over union: A metric and a loss for bounding box regression. \n",
        "    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 658-666).\n",
        "    \n",
        "    ----------\n",
        "    Input\n",
        "        y_true: the target bounding box. \n",
        "        y_pred: the predicted bounding box.\n",
        "        \n",
        "        Elements of a bounding box should be organized as: [y_min, x_min, y_max, x_max].\n",
        "        mode: 'iou' for IoU coeff (i.e., Jaccard index);\n",
        "              'giou' for generalized IoU coeff.\n",
        "        \n",
        "        dtype: the data type of input tensors.\n",
        "               Default is tf.float32.\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    y_pred = tf.cast(y_pred, dtype)\n",
        "    \n",
        "    y_true = tf.cast(y_true, dtype)\n",
        "    \n",
        "    y_pred = tf.squeeze(y_pred)\n",
        "    y_true = tf.squeeze(y_true)\n",
        "\n",
        "    return 1 - iou_box_coef(y_true, y_pred, mode=mode, dtype=dtype)\n",
        "\n",
        "\n",
        "def iou_seg(y_true, y_pred, dtype=tf.float32):\n",
        "    \"\"\"\n",
        "    Inersection over Union (IoU) loss for segmentation maps. \n",
        "    \n",
        "    iou_seg(y_true, y_pred, dtype=tf.float32)\n",
        "    \n",
        "    ----------\n",
        "    Rahman, M.A. and Wang, Y., 2016, December. Optimizing intersection-over-union in deep neural networks for \n",
        "    image segmentation. In International symposium on visual computing (pp. 234-244). Springer, Cham.\n",
        "    \n",
        "    ----------\n",
        "    Input\n",
        "        y_true: segmentation targets, c.f. `keras.losses.categorical_crossentropy`\n",
        "        y_pred: segmentation predictions.\n",
        "        \n",
        "        dtype: the data type of input tensors.\n",
        "               Default is tf.float32.\n",
        "        \n",
        "    \"\"\"\n",
        "\n",
        "    # tf tensor casting\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    y_pred = tf.cast(y_pred, dtype)\n",
        "    y_true = tf.cast(y_true, y_pred.dtype)\n",
        "\n",
        "    y_pred = tf.squeeze(y_pred)\n",
        "    y_true = tf.squeeze(y_true)\n",
        "    \n",
        "    y_true_pos = tf.reshape(y_true, [-1])\n",
        "    y_pred_pos = tf.reshape(y_pred, [-1])\n",
        "\n",
        "    area_intersect = tf.reduce_sum(tf.multiply(y_true_pos, y_pred_pos))\n",
        "    \n",
        "    area_true = tf.reduce_sum(y_true_pos)\n",
        "    area_pred = tf.reduce_sum(y_pred_pos)\n",
        "    area_union = area_true + area_pred - area_intersect\n",
        "    \n",
        "    return 1-tf.math.divide_no_nan(area_intersect, area_union)\n",
        "\n",
        "# ========================= #\n",
        "# Semi-hard triplet\n",
        "\n",
        "def triplet_1d(y_true, y_pred, N, margin=5.0):\n",
        "    \n",
        "    '''\n",
        "    (Experimental)\n",
        "    Semi-hard triplet loss with one-dimensional vectors of anchor, positive, and negative.\n",
        "    \n",
        "    triplet_1d(y_true, y_pred, N, margin=5.0)\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        y_true: a dummy input, not used within this function. Appeared as a requirment of tf.keras.loss function format.\n",
        "        y_pred: a single pass of triplet training, with `shape=(batch_num, 3*embeded_vector_size)`.\n",
        "                i.e., `y_pred` is the ordered and concatenated anchor, positive, and negative embeddings.\n",
        "        N: Size (dimensions) of embedded vectors\n",
        "        margin: a positive number that prevents negative loss.\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    # anchor sample pair separations.\n",
        "    Embd_anchor = y_pred[:, 0:N]\n",
        "    Embd_pos = y_pred[:, N:2*N]\n",
        "    Embd_neg = y_pred[:, 2*N:]\n",
        "    \n",
        "    # squared distance measures\n",
        "    d_pos = tf.reduce_sum(tf.square(Embd_anchor - Embd_pos), 1)\n",
        "    d_neg = tf.reduce_sum(tf.square(Embd_anchor - Embd_neg), 1)\n",
        "    loss_val = tf.maximum(0., margin + d_pos - d_neg)\n",
        "    loss_val = tf.reduce_mean(loss_val)\n",
        "    \n",
        "    return loss_val"
      ],
      "metadata": {
        "id": "aVjFxkqM8L7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "opYscUjk8MAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testImages=TestX\n",
        "\n",
        "testImages.shape"
      ],
      "metadata": {
        "id": "cHeSlmDG8MEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(\"E:/Dataset_test.h5\", 'w') as hdf:\n",
        "    hdf.create_dataset('images', data=testImages, compression='gzip', compression_opts=9)"
      ],
      "metadata": {
        "id": "8VNxlYt58MHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = h5py.File('E:/Dataset_test.h5', 'r')\n",
        "imgs_test = file.get('images')\n",
        "#imgs_mask_test = file.get('masks')\n",
        "imgs_test = np.array(imgs_test)\n",
        "#imgs_mask_test = np.array(imgs_mask_test)\n",
        "imgs_test = imgs_test.astype('float32')\n",
        "imgs_test -= mean\n",
        "imgs_test /= std\n",
        "\n",
        "print('*'*30)\n",
        "print('Loading saved weights...')\n",
        "print('*'*30)\n",
        "model.load_weights('TransUnet++-Amazon.h5')\n",
        "\n",
        "print('*'*30)\n",
        "print('Predicting masks on test data...')\n",
        "print('*'*30)\n",
        "imgs_mask_test = model.predict(imgs_test, verbose=1,batch_size=1)\n",
        "imgs_mask_test=(imgs_mask_test - np.min(imgs_mask_test))/(np.max(imgs_mask_test) - np.min(imgs_mask_test))\n"
      ],
      "metadata": {
        "id": "2HICFgxD8MKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestY=TestY.reshape(TestY.shape[0],TestY.shape[1],TestY.shape[1],1)"
      ],
      "metadata": {
        "id": "PNqppAWT8bb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = model.evaluate(imgs_test, TestY, batch_size=1)"
      ],
      "metadata": {
        "id": "af8oUeRr8bh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_mask_test[imgs_mask_test<=0.5]=0\n",
        "imgs_mask_test[imgs_mask_test>=0.5]=1\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay \n",
        "\n",
        "# Confusion Matrix\n",
        "def confusion_matrix(y_true, y_pred, num_classes=None):\n",
        "    y_true = y_true.reshape(-1)\n",
        "    y_pred = y_pred.reshape(-1) \n",
        "    if (num_classes is None):\n",
        "        num_classes = max(max(y_true), max(y_pred)) + 1\n",
        "    cm = num_classes * y_true + y_pred\n",
        "    cm = np.bincount(cm, minlength=num_classes*num_classes)\n",
        "    cm = cm.reshape(num_classes, num_classes)\n",
        "    return cm\n",
        "\n",
        "cm = confusion_matrix(TestY.astype('int64'),imgs_mask_test.astype('int64'))\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize = (5,5))\n",
        "\n",
        "\n",
        "cm_display = ConfusionMatrixDisplay(cm)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "cm_display.plot(ax=ax)\n",
        "plt.savefig(\"E:/TransUNet++-Amazon400-cm.png\",facecolor='w', dpi=300)"
      ],
      "metadata": {
        "id": "vDaGaMgD8bmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSqyzBu98-5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('*' * 30)\n",
        "print('Saving predicted masks to files...')\n",
        "print('*' * 30)\n",
        "pred_dir = 'E:/Predictions-TransUnet++-Amazon'\n",
        "if not os.path.exists(pred_dir):\n",
        "    os.mkdir(pred_dir)\n",
        "for i, image in enumerate(imgs_mask_test):\n",
        "    #image = (image * 255).astype(np.uint8)\n",
        "    \n",
        "    image=(image - np.min(image))/(np.max(image) - np.min(image))\n",
        "    image = (image * 255).astype(np.uint8)\n",
        "    \n",
        "    \n",
        "    cv2.imwrite(os.path.join(pred_dir, str(i + 1) + '_pred.png'), image)"
      ],
      "metadata": {
        "id": "X66A46qM8-9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDzdHLPc8_BY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}